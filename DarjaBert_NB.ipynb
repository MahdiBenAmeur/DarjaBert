{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this notebook im attempting to build an nlp model capable of predicted the missing tokens in a piece of text written IN THE TUNISIAN DARIJA , both with latin and arabic alphabet , ofcourse the model we be later on be used as a base model for other tasks (word infilling/generation , classification, question answering, named entity recognition, translation, sentiment analysis etc...)\n",
    "for learning purposes im going to try to write every function from scratch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing the necessary libraries (there will be more later on)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import tqdm\n",
    "import json\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## downloading and exploring the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = load_dataset(\"khaled123/Tunisian_Dialectic_English_Derja\" , cache_dir= \"darija_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1664765\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ds[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: {'text': 'منجية تراها تلحق بها لتسألها عن ابنتها مشهد نوفمبر آخر الليل داخلي المستشفى قاعة التمريض منجية ممرضة ممرضة يوجد بالقاعة'} \n",
      "Example 2: {'text': 'إي أكهو خليها'} \n"
     ]
    }
   ],
   "source": [
    "number_of_exemples= 2\n",
    "for i in range(number_of_exemples):\n",
    "    random_index = random.randint(0, len(train_data))\n",
    "    print(f\"Example {i+1}: {train_data[random_index]} \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing everythin other than alphanumeric characters\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text[\"text\"] = re.sub(r'[^\\u0600-\\u06FFa-zA-Z0-9\\s]', '', text[\"text\"])\n",
    "    text[\"text\"] = text[\"text\"].lower()\n",
    "    text[\"text\"] = ' '.join(text[\"text\"].split())\n",
    "\n",
    "    return text\n",
    "\n",
    "cleaned_data = train_data.map(clean_text)\n",
    "cleaned_data = cleaned_data.filter(lambda x: x[\"text\"].strip() != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned 1: {'text': 'question اكتب تغريدة إيجابية answer الخطوة 1 حدد موضوع عام إيجابي باش نختار الامتنان كموضوع إيجابي للتغريدة خاطر يشجع الناس على تقدير الحاجات المليحة في حياتهم وينشر السعادة الخطوة 2 اختر تركيز محدد مهم إنو التغريدة تكون قريبة للناس، لذلك باش نركز على الأفراح اليومية والإنجازات اللي ينجموا الناس يتعاطفوا معاها ويعترفوا بيها الخطوة 3 بناء التغريدة باش نستعمل لغة غير رسمية ونخلي التغريدة قصيرة، خاطر التغريدات عندها حد أقصى من الحروف 280 إضافة الرموز التعبيرية تنجم تعزز الرسالة الإيجابية الخطوة 4 مراجعة وإنهاء التغريدة بعد المراجعة، نتأكد إنو التغريدة تثير شعور إيجابي وعندها القدرة على التفاعل مع جمهور واسع التغريدة النهائية اليوم، خلينا نحتفل بالنصر الصغير ونعتز بلحظات الفرح تذكروا إنكم تكونوا ممتنين للحاجات الصغيرة اللي تخلي الحياة جميلة امتنان إيجابية'} \n",
      "----------------------------------------------------\n",
      "cleaned 2: {'text': 'sure lets break down the translation stepbystep step 1 translation the original tunisian dialect text is الفليجة اللي خذاتها ما عجبتنيش مشيت بدلتهالها 1 الفليجة the flija flija refers to a type of tunisian pastry or dish 2 اللي that a relative pronoun 3 خذاتها i took it from the verb خذ which means to take 4 ما عجبتنيش i didnt like it the negation ما means not and عجبتني means it pleased me 5 مشيت i went from the verb يمشي which means to go 6 بدلتهالها i changed it for her from بدل meaning to change and لها meaning for her putting it all together the translation would be the flija that i took i didnt like it so i went and changed it for her step 2 explanation of translation choices tunisian dialect vs standard arabic tunisian arabic or derja has many unique words and phrases that differ from modern standard arabic msa for example الفليجة is a specific term that may not be widely recognized in msa the structure of sentences can also vary with more informal and colloquial expressions used in dialects capturing essence the phrase ما عجبتنيش is a colloquial way of expressing dislike which is more direct and informal than the msa equivalent the use of مشيت instead of the msa ذهبت reflects the everyday speech pattern in tunisian arabic overall the translation aims to maintain the casual tone and meaning of the original text while making it understandable in english if you have any more questions or need further assistance feel free to ask'} \n",
      "----------------------------------------------------\n",
      "cleaned 3: {'text': 'sure lets break down the translation stepbystep original text ويبشر بكل خير على كل هاذا نوع من الحوارات إلي تهم الشباب على ما يبدو وما فماش مانع لو ندرجوها stepbystep translation 1 ويبشر بكل خير translation and he brings good news explanation ويبشر wa yubashir means and he brings good news or and he announces good news this phrase sets a positive tone 2 على كل هاذا نوع من الحوارات translation about all this type of dialogues explanation على ala means about كل kul means all هاذا hatha is a colloquial form of هذا hatha meaning this and نوع من الحوارات naw min alhiwarat translates to type of dialogues the phrase refers to a specific category of discussions 3 إلي تهم الشباب على ما يبدو translation that seem to concern the youth explanation إلي ili is a colloquial form of التي allati meaning that تهم tuhimm means concern and الشباب alshabab means the youth على ما يبدو ala ma yabdu translates to it seems indicating a perception or observation 4 وما فماش مانع لو ندرجوها translation and there is no objection if we include them explanation وما فماش wa ma famaash means and there is no مانع mani means objection and لو ندرجوها law nadrijuha translates to if we include them this part suggests openness to including these dialogues full translation and he brings good news about all this type of dialogues that seem to concern the youth and there is no objection if we include them explanation of translation choices tunisian dialect vs standard arabic tunisian arabic darija often incorporates colloquial expressions and simplified grammar compared to'} \n",
      "----------------------------------------------------\n",
      "cleaned 4: {'text': 'sure lets break down the translation stepbystep step 1 translation the text أما هوما امراة كبيرة وراجل كبير يسخفوا can be translated to english as but they are an old woman and an old man who are joking step 2 explanation of translation choices 1 أما ama this word translates to but which is a common conjunction used to contrast ideas 2 هوما homa this is a colloquial way of saying they in tunisian dialect which differs from the standard arabic هم hum 3 امراة كبيرة imraa kbira this means old woman امراة imraa is the word for woman and كبيرة kbira means old or big in standard arabic it would be امرأة كبيرة imraah kabirah but the pronunciation and some vowel markings differ in the dialect 4 وراجل كبير wrajel kbira this translates to old man راجل rajel is the word for man and كبير kbira again means old in standard arabic it would be رجل كبير rajul kabir 5 يسخفوا yaskhafu this verb means to joke or to make fun in standard arabic the verb would be يُهَزِأ yuhziu or يَمزَح yamza but in tunisian dialect يسخف yaskhaf is more commonly used summary the translation captures the essence of the original text conveying that there are an old woman and an old man who are engaging in joking or playful behavior the tunisian dialect has its unique vocabulary and pronunciation which can differ significantly from standard arabic but the core meaning remains intact in the translation if you have any more questions or need further assistance feel free to ask'} \n",
      "----------------------------------------------------\n",
      "cleaned 5: {'text': 'اللوم موش عليك وحدك أما على هاك البهايم اللي قاعدين و يا راجل لا تمشي في لحظة ما يرحمكشي فيها ربي'} \n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "number_of_exemples= 5\n",
    "for i in range(number_of_exemples):\n",
    "    random_index = random.randint(0, len(cleaned_data))\n",
    "    print(f\"cleaned {i+1}: {cleaned_data[random_index]} \")\n",
    "    print(\"----------------------------------------------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets try tokenizing the dataset !\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ofcourse there is multiple  of tokenization algorithms , like BPE , word tokenization ,  WordPiece , and sentence tokenization ect .\n",
    "\n",
    "for the first try im going to go with word tokenizaiton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict={}\n",
    "repetition_dict={}\n",
    "\n",
    "def tokenize_Data_by_word (text):    \n",
    "    phrase = text[\"text\"].lower().split()\n",
    "    result = []\n",
    "    for word in phrase:\n",
    "        if word in vocab_dict:\n",
    "            result.append(vocab_dict[word])\n",
    "            repetition_dict[word]+=1\n",
    "        else:\n",
    "            vocab_dict[word] = len(vocab_dict)\n",
    "            result.append(vocab_dict[word])\n",
    "            repetition_dict[word]=0\n",
    "    return {\"text\" :result}\n",
    "\n",
    "tokenized_data = cleaned_data.map(tokenize_Data_by_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_dict) \n",
    "len(repetition_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index , (key, value )in enumerate(vocab_dict.items()):\n",
    "    print(f\"key : {key} , value : {value} , rep : {repetition_dict[key]}\")\n",
    "    if index >10 :\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can clearly see with this method the vocab dictionary gets wayy wayyy too bigg due to our method and to our data having both arabic and english and also multiple prounonciations and type of writting for the same words, not efficient for our purposes and also does not handle the case where unkown words are entered , huge issue\n",
    "lets try implementing another methode , specifically\n",
    "\n",
    "## bite pair encoding (bpe)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first step lets create our initial vocab dictionary , it will contain the basic caracteres and numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, 'ا': 26, 'ب': 27, 'ت': 28, 'ث': 29, 'ج': 30, 'ح': 31, 'خ': 32, 'د': 33, 'ذ': 34, 'ر': 35, 'ز': 36, 'س': 37, 'ش': 38, 'ص': 39, 'ض': 40, 'ط': 41, 'ظ': 42, 'ع': 43, 'غ': 44, 'ف': 45, 'ق': 46, 'ك': 47, 'ل': 48, 'م': 49, 'ن': 50, 'ه': 51, 'و': 52, 'ي': 53, '0': 54, '1': 55, '2': 56, '3': 57, '4': 58, '5': 59, '6': 60, '7': 61, '8': 62, '9': 63, ' ': 64, '[UNK]': 65, '[CLS]': 66, '[SEP]': 67, '[MASK]': 68, '[PAD]': 69}\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {}\n",
    "\n",
    "english_letters = [chr(i) for i in range(ord('a'), ord('z') + 1)]\n",
    "\n",
    "arabic_letters = [\n",
    "    'ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ',\n",
    "    'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص',\n",
    "    'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق',\n",
    "    'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي'\n",
    "]\n",
    "digits = [str(i) for i in range(10)]\n",
    "\n",
    "all_chars = english_letters + arabic_letters + digits\n",
    "\n",
    "for idx, char in enumerate(all_chars):\n",
    "    vocab_dict[char] = idx\n",
    "vocab_dict[\" \"] = 64\n",
    "vocab_dict[\"[UNK]\"] =65\n",
    "vocab_dict[\"[CLS]\"] =66\n",
    "vocab_dict[\"[SEP]\"] =67\n",
    "vocab_dict[\"[MASK]\"] =68\n",
    "vocab_dict[\"[PAD]\"] =69\n",
    "\n",
    "print(vocab_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_dict) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ofcourse that not our vocab dictionary but its base for now , we need build our vocab based on our training data , we will specify the size to be 20000 (it will stop around that number but not specificly at that number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 20000\n",
    "def tokenize(text , vocab_dict=vocab_dict , ignore_unknown=False):\n",
    "    text = text.lower()\n",
    "    tokens =[]\n",
    "    encoding =[]\n",
    "    i=0\n",
    "    MAX_SUBWORD_LEN = 8\n",
    "\n",
    "    while(i<len(text)):\n",
    "        found = False\n",
    "        for j in range(min(i + MAX_SUBWORD_LEN, len(text)), i, -1):\n",
    "            subword = text[i:j]\n",
    "            if subword in vocab_dict:\n",
    "                tokens.append(subword)\n",
    "                encoding.append(vocab_dict[subword])\n",
    "                found = True\n",
    "                i=j\n",
    "                break\n",
    "        if not found :\n",
    "            i+=1\n",
    "            if  ignore_unknown==False:\n",
    "                tokens.append(\"[UNK]\")\n",
    "                encoding.append(vocab_dict[\"[UNK]\"])\n",
    "    return tokens , encoding\n",
    "            \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['c', 'h', 'a', 'p', 'p', 'a', 't', 'y', ' ', 'i', 's', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 't', 'a', 's', 't', 'y', 'y'], [2, 7, 0, 15, 15, 0, 19, 24, 64, 8, 18, 64, 17, 4, 0, 11, 11, 24, 64, 19, 0, 18, 19, 24, 24])\n",
      "(['ص', 'ب', 'ا', 'ح', ' ', 'ا', 'ل', 'خ', 'ي', 'ر'], [39, 27, 26, 31, 64, 26, 48, 32, 53, 35])\n",
      "(['[UNK]', 'c', 'l', 's', '[UNK]', ' ', ' ', ' ', ' ', 'ص', 'ب', 'ا', 'ح', ' ', 'ا', 'ل', 'خ', 'ي', 'ر'], [65, 2, 11, 18, 65, 64, 64, 64, 64, 39, 27, 26, 31, 64, 26, 48, 32, 53, 35])\n"
     ]
    }
   ],
   "source": [
    "phrase1 = \"chappaty is really tastyy\"\n",
    "phrase2 = \"صباح الخير\"\n",
    "print(tokenize( phrase1))\n",
    "print(tokenize(phrase2))\n",
    "print(tokenize(\"[CLS]    \"+phrase2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the below algorithm works as the following , first we get a big chunck of continous text , tokenize it with our current vocab dictionary , find the top 500 frequent token pair that are over the specified minimum frequency , add them to the vocab dictionary , and repeat , untill we have over 20000 tokens in the vocab dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets take top 500 repeting tokens\n",
    "num_token_to_take = 500\n",
    "min_frequency= 500\n",
    "max_vocab_size = 20000\n",
    "\n",
    "def add_to_vocab(chunk):\n",
    "    phrase = chunk.lower()\n",
    "    tokens , encoding = tokenize(phrase)\n",
    "    #print(\"generated tokens\")\n",
    "    #coupeling\n",
    "    temp= {}\n",
    "    for i in (range(0,len(tokens))):\n",
    "        if i+1 > len(tokens)-1 :\n",
    "            break\n",
    "        if tokens[i] == \"[UNK]\" or tokens[i+1] == \"[UNK]\" or len(tokens[i+1])+len(tokens[i])>8:\n",
    "            continue\n",
    "\n",
    "        pair = tokens[i]+tokens[i+1]\n",
    "        if pair in temp:\n",
    "            temp[pair]+=1\n",
    "        else:\n",
    "            temp[pair]=1\n",
    "    sorted_new_tokens = sorted(temp.items(), key=lambda x: x[1], reverse=True)\n",
    "    #lets take top 500 repeting tokens\n",
    "    added=0\n",
    "    for  new_token , freq in  sorted_new_tokens:\n",
    "        if freq<min_frequency:\n",
    "            break\n",
    "        if new_token not in vocab_dict:\n",
    "            vocab_dict[new_token] = len(vocab_dict)\n",
    "            added+=1\n",
    "        if added>=num_token_to_take:\n",
    "            break\n",
    " \n",
    "#TO PASS the data as big chunks of text\n",
    "def get_chunk(data, chunk_size=150_000_000):\n",
    "    current_chunk = []\n",
    "    current_len = 0\n",
    "\n",
    "    for example in data:\n",
    "        text = example[\"text\"] + \" \"\n",
    "        current_chunk.append(text)\n",
    "        current_len += len(text)\n",
    "\n",
    "        if current_len >= chunk_size:\n",
    "            yield \"\".join(current_chunk)\n",
    "            current_chunk = []\n",
    "            current_len = 0\n",
    "\n",
    "    if current_chunk:\n",
    "        yield \"\".join(current_chunk)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returned : 150001400\n"
     ]
    }
   ],
   "source": [
    "gen = get_chunk(cleaned_data)\n",
    "print(f\"returned : {len(next(gen))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUILDING THE VOCAB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_iter = 100\n",
    "iter = 0\n",
    "while (len(vocab_dict) < max_vocab_size):\n",
    "    if iter>=max_iter:\n",
    "        break\n",
    "    for i in tqdm.tqdm(get_chunk(cleaned_data)):\n",
    "            \n",
    "        iter += 1\n",
    "\n",
    "        print(f\"len of chunk : {len(i)}\")\n",
    "        old_vocab_size = len(vocab_dict)\n",
    "        add_to_vocab(i)\n",
    "        print(f\" number of added tokens : {len(vocab_dict)-old_vocab_size} ,new vocab size : {len(vocab_dict)}\")\n",
    "        if len(vocab_dict) >= max_vocab_size:\n",
    "            break\n",
    "\n",
    "    print(\"full circle\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see some exemple tokens!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample = random.sample(list(vocab_dict.items()), 20)\n",
    "\n",
    "for token, idx in sample:\n",
    "    print(f\"{repr(token):<10} -> {idx}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving our vocabulary dict for future use\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will need to save and retrive our vocab dict later on so lets build a save and load functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab_dict(path , vocab_dict):\n",
    "    with open(path, 'w' , encoding=\"utf-8\") as f:\n",
    "        json.dump(vocab_dict,f ,ensure_ascii=False)\n",
    "def load_vocab_dict(path ):\n",
    "    with open(path , \"r\" , encoding=\"utf-8\") as f:\n",
    "        vocab = json.load(f)\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = \"./vocab_dict.json\"\n",
    "save_vocab_dict(path, vocab_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new vocab size : 20153\n"
     ]
    }
   ],
   "source": [
    "path  = \"./vocab_dict.json\"\n",
    "\n",
    "new_vocab = load_vocab_dict(path)\n",
    "\n",
    "print(f\"new vocab size : {len(new_vocab)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pment'    -> 13223\n",
      "' موجود'   -> 2822\n",
      "' دور'     -> 2559\n",
      "'دوش '     -> 18034\n",
      "' بدون '   -> 14522\n",
      "' cra'     -> 17341\n",
      "'end'      -> 3910\n",
      "'ينجم'     -> 2197\n",
      "'خط'       -> 2088\n",
      "'ath '     -> 8618\n",
      "'cle '     -> 9450\n",
      "' imagin'  -> 9565\n",
      "' wea'     -> 10200\n",
      "'متاز'     -> 14575\n",
      "'في عالم'  -> 19312\n",
      "'وضعي'     -> 5064\n",
      "' نشر'     -> 13210\n",
      "'loved '   -> 12644\n",
      "'مرتب'     -> 2988\n",
      "' الرا'    -> 14220\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample = random.sample(list(new_vocab.items()), 20)\n",
    "\n",
    "for token, idx in sample:\n",
    "    print(f\"{repr(token):<10} -> {idx}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have built and saved the tokenizer and vocabulary , lets build the decoder now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_vocab = {v:k for k , v in new_vocab.items()}\n",
    "def decode (tokens):\n",
    "    sentence = [token_to_vocab.get(token,\"\") for token in tokens]\n",
    "    return \"\".join(sentence)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Latin Phrase\n",
      "Original: Wenek? men bekry nestanew fyyk\n",
      "Tokens  : ['we', 'ne', 'k', ' men', ' be', 'k', 'ry', ' ne', 'stan', 'ew ', 'fy', 'yk']\n",
      "Encoding: [318, 208, 10, 12354, 3918, 10, 340, 1389, 600, 6413, 6790, 15604]\n",
      "Decoded : wenek men bekry nestanew fyyk\n",
      "\n",
      "🔠 Arabic Phrase\n",
      "Original: وينك؟ من بكري نستناو فيك\n",
      "Tokens  : ['وين', 'ك', ' من ', 'بكري', ' نست', 'نا', 'و في', 'ك']\n",
      "Encoding: [19383, 47, 1572, 6303, 2408, 180, 1844, 47]\n",
      "Decoded : وينك من بكري نستناو فيك\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_phrase_in_darija = \"Wenek? men bekry nestanew fyyk\"\n",
    "test_phrase_in_arabic_darija = \"وينك؟ من بكري نستناو فيك\"\n",
    "\n",
    "\n",
    "tokens1, encoding1 = tokenize(test_phrase_in_darija , vocab_dict=new_vocab,ignore_unknown=True)\n",
    "tokens2, encoding2 = tokenize(test_phrase_in_arabic_darija, vocab_dict=new_vocab , ignore_unknown=True)\n",
    "\n",
    "print(\"🔤 Latin Phrase\")\n",
    "print(\"Original:\", test_phrase_in_darija)\n",
    "print(\"Tokens  :\", tokens1)\n",
    "print(\"Encoding:\", encoding1)\n",
    "print(\"Decoded :\", decode(encoding1))\n",
    "\n",
    "print(\"\\n🔠 Arabic Phrase\")\n",
    "print(\"Original:\", test_phrase_in_arabic_darija)\n",
    "print(\"Tokens  :\", tokens2)\n",
    "print(\"Encoding:\", encoding2)\n",
    "print(\"Decoded :\", decode(encoding2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we created the Vocabulary, Tokenizer, Encoder & Decoder , Save and Load\n",
    "\n",
    "everything looks great, onto the next big step\n",
    "\n",
    "---\n",
    "\n",
    "# LET BUILD THE MODDEEELLLLL\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im basing it on the transformers architecture, with only an encoder for now, like BERT  \n",
    "i might try a different architecture later on\n",
    "\n",
    "heres the orginal paper for refrence:  \n",
    "https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "\n",
    "---\n",
    "\n",
    "first , lets start by creating the embadding and positionel embadding layer first\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building the embadding layer from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embadding layer but from scratch\n",
    "class DarjaEmbadding(nn.Module):\n",
    "    def __init__(self, vocab_size, embad_dim , window_size):\n",
    "        super().__init__()\n",
    "        self.embaddings = nn.Parameter(torch.randn((vocab_size, embad_dim)),requires_grad=True)\n",
    "        self.positionel_embaddings = nn.Parameter(torch.randn((window_size, embad_dim)),requires_grad=True)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.embaddings[tokens]+self.positionel_embaddings[:tokens.shape[1]]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to help us test the embedding model , we should creating a function that takes a list of encodings and turns it into a input tensor of\n",
    "shape [B, T] to pass onto the model\n",
    "where B is the batch size , and T is the number of tokens per tensor\n",
    "\n",
    "-note : T should be the same of all the passed vectors , so we might need to expend some vectors (add paddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(encodings, vocab_dict , window_size):\n",
    "    batch = []\n",
    "    for encoding in encodings:\n",
    "        batch.append(encoding)\n",
    "    max_len_phrase = len(max(batch , key=lambda x : len(x)))\n",
    "\n",
    "    if max_len_phrase> window_size:\n",
    "        raise ValueError(f\"Sequence exceeds window size of {window_size}\")\n",
    "    #kabber el encoding \n",
    "    new_batch= []\n",
    "    for encoding in batch:\n",
    "        encoding = encoding + [vocab_dict[\"[PAD]\"]]*(max_len_phrase-len(encoding))\n",
    "        new_batch.append(encoding)\n",
    "\n",
    "\n",
    "    new_batch = torch.tensor(new_batch,dtype = torch.long)\n",
    "    return new_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 318,  208,  305,  410,  119,   69,   69,   69],\n",
      "        [ 145,  553, 2217,  132, 6917, 6003, 8526,    0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6020,  0.8431,  2.3258,  ..., -1.8662,  0.6345, -0.9976],\n",
       "         [ 1.2322,  0.0977,  1.3831,  ..., -0.0959,  0.2493, -0.9996],\n",
       "         [ 1.2748, -0.2016, -2.6916,  ..., -1.7636, -1.0816, -0.7561],\n",
       "         ...,\n",
       "         [-1.3675,  1.7508, -0.0963,  ..., -0.6348,  1.0943, -0.4093],\n",
       "         [-0.3096,  0.1913, -0.3836,  ..., -1.7507,  1.0601,  0.3299],\n",
       "         [-0.3802,  0.7613, -1.0469,  ..., -0.8571,  2.7972,  0.6922]],\n",
       "\n",
       "        [[-0.5574,  2.8418,  0.0504,  ..., -0.5606, -0.9841, -1.3850],\n",
       "         [ 1.3299, -1.1614,  0.9689,  ...,  0.4555, -1.0745, -0.8468],\n",
       "         [ 0.7334, -0.1789,  0.8090,  ..., -0.7477, -0.8592, -0.6266],\n",
       "         ...,\n",
       "         [-1.0546,  0.1370, -1.0977,  ...,  2.5629, -2.2278,  0.6302],\n",
       "         [-0.0725, -0.0287, -1.2213,  ...,  0.2969, -0.7501,  1.5872],\n",
       "         [-0.3932, -0.1261, -1.1436,  ...,  0.4723, -1.0987,  1.0220]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#small test\n",
    "vocab_size = len(new_vocab)\n",
    "embad_dim = 512\n",
    "window_size = 2048\n",
    "embaddinglayer = DarjaEmbadding(vocab_size, embad_dim , window_size )\n",
    "phrase = \"wenek bro ?\"\n",
    "phrase2=\"hanyyy fythnyaa\"\n",
    "_ , encoding1 = tokenize(phrase,new_vocab ,True)\n",
    "_ , encoding2 = tokenize(phrase2,new_vocab ,True)\n",
    "\n",
    "batch = to_tensor([encoding1,encoding2] ,new_vocab , window_size )\n",
    "print(batch)\n",
    "result = embaddinglayer(batch)\n",
    "result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by enspecting the resulting matrix we can see that we have a shape of B T D . B being the batch size T being the number of tokens per tensor ( Padded to be the smae for each tensor) , and newly D which is the embadding Dimention for each token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking Function\n",
    "\n",
    "now lets build the masking function  \n",
    "the maskign function will take an input phrase (or batch of phrases in our case)  \n",
    "and randomly replace some tokens with the [mask] token  \n",
    "this will be indecat to our model that it will try to generate the missing tokens based on the context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original  : he made my day\n",
      "tokens    : ['he made ', 'my d', 'ay']\n",
      "encoding  : [19716, 5347, 243]\n",
      "original  : احلامي بسيييييطة\n",
      "tokens    : ['احل', 'امي ', 'بس', 'يييي', 'يط', 'ة']\n",
      "encoding  : [13511, 10407, 2955, 15092, 6449, 20145]\n",
      "original  : عاودولي الصيف النفسية ماهياش جاهزة لتغليف الكتب و الpanier و ال autodicte\n",
      "tokens    : ['عاود', 'ولي ', 'الصيف ', 'النفسي', 'ة', ' ما', 'هياش ', 'جاهز', 'ة', ' لت', 'غل', 'يف ', 'الك', 'تب', ' و ال', 'pa', 'ni', 'er', ' و ال', ' a', 'ut', 'od', 'ic', 'te']\n",
      "encoding  : [6165, 1970, 19497, 19860, 20145, 1845, 12589, 13995, 20145, 17953, 15261, 15342, 1918, 507, 8837, 369, 169, 104, 8837, 77, 287, 394, 122, 100]\n",
      "original  : عكس ما يقولوا فيروز في الليل خير\n",
      "tokens    : ['عكس ', 'ما ي', 'قولوا ', 'في', 'رو', 'ز في ', 'الليل ', 'خير']\n",
      "encoding  : [9068, 1646, 13262, 151, 355, 15640, 13437, 1871]\n",
      "original  : كيف نتحمّس عادة النتيجة\n",
      "tokens    : ['كيف ', 'نت', 'حم', 'س ', 'عاد', 'ة', ' النتيج', 'ة']\n",
      "encoding  : [1838, 302, 1863, 261, 1864, 20145, 11430, 20145]\n"
     ]
    }
   ],
   "source": [
    "#small modification to get chunk function to return a list of phrases instead of a one single long phrase\n",
    "def get_chunk(data, chunk_lenght=5 , start = 0):\n",
    "    current_chunk = []\n",
    "    current_len = 0\n",
    "    for pos  , example in enumerate(data):\n",
    "        if pos < start:\n",
    "            continue\n",
    "        text = example[\"text\"] \n",
    "        if len(text.split())<2:\n",
    "            continue\n",
    "        current_chunk.append(text)\n",
    "        current_len+=1\n",
    "        if current_len >= chunk_lenght:\n",
    "            yield current_chunk\n",
    "            current_chunk = []\n",
    "            current_len = 0\n",
    "\n",
    "    if current_chunk:\n",
    "        yield current_chunk\n",
    "#modification to tokenize function to tokenize by batch\n",
    "def tokenize_batch(chunk , vocab_dict ,trunc = float(\"inf\"), ignore_unk = False ):\n",
    "    tokens = []\n",
    "    encodings = []\n",
    "    for text in chunk:\n",
    "        token , encoding = tokenize(text , vocab_dict=vocab_dict, ignore_unknown = ignore_unk)\n",
    "        if len(encoding)> trunc:\n",
    "            encoding = encoding[:trunc]\n",
    "            token = token[:trunc]\n",
    "        tokens.append(token)\n",
    "        encodings.append(encoding)\n",
    "    return tokens, encodings\n",
    "\n",
    "#test \n",
    "chunk = next(get_chunk(cleaned_data))\n",
    "\n",
    "tokens , encoding = tokenize_batch(chunk , new_vocab ,ignore_unk= True)\n",
    "\n",
    "for i in range ( len(chunk)):\n",
    "    print(f\"original  : {chunk[i]}\")\n",
    "    print(f\"tokens    : {tokens[i]}\")\n",
    "    print(f\"encoding  : {encoding[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to prevent overfitting one the [mask] token we are going to randomly replace the selected token with another random one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "            \n",
    "#after tokenization we will pass the tokens to this function to mask random tokens\n",
    "def mask_batch(list_of_tokens , list_of_encoding ,vocab_dict = new_vocab , mask_rate = 0.15):\n",
    "    \n",
    "    result_tokens = []\n",
    "    result_encodings = []\n",
    "    special_tokens= [\"[UNK]\" , \"[CLS]\" , \"[SEP]\" , \"[MASK]\" , \"[PAD]\"]\n",
    "\n",
    "    candidate_tokens = [tok for tok in vocab_dict if tok not in special_tokens]\n",
    "\n",
    "    result_indexes = []\n",
    "    for index in range(len(list_of_tokens)):\n",
    "        masked_tokens=list(list_of_tokens[index])\n",
    "        masked_encodings=list(list_of_encoding[index])\n",
    "        nb_tokens = len(masked_tokens)\n",
    "        nb_tokens_to_mask = int(nb_tokens*mask_rate)+1\n",
    "        random_indexes = random.sample(range(0, nb_tokens), nb_tokens_to_mask)\n",
    "        for i in random_indexes:\n",
    "            replace= random.random()\n",
    "            if replace >0.15 :\n",
    "                masked_tokens[i]= \"[MASK]\"\n",
    "                masked_encodings[i]= vocab_dict[\"[MASK]\"]\n",
    "            else :\n",
    "                random_token = random.choice(candidate_tokens)\n",
    "                masked_tokens[i]= random_token\n",
    "                masked_encodings[i]= vocab_dict[random_token]\n",
    "        result_tokens.append(masked_tokens)\n",
    "        result_encodings.append(masked_encodings)\n",
    "        result_indexes.append(random_indexes)\n",
    "    return result_tokens , result_encodings , result_indexes\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i decide to to make a second version that can mask a sequence of tokens , but both work chose what youd like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after tokenization we will pass the tokens to this function to mask random tokens\n",
    "def mask_batch_seq(list_of_tokens , list_of_encoding ,vocab_dict = new_vocab , mask_rate = 0.15):\n",
    "    \n",
    "    result_tokens = []\n",
    "    result_encodings = []\n",
    "    special_tokens= [\"[UNK]\" , \"[CLS]\" , \"[SEP]\" , \"[MASK]\" , \"[PAD]\"]\n",
    "\n",
    "    candidate_tokens = [tok for tok in vocab_dict if tok not in special_tokens]\n",
    "\n",
    "    result_indexes = []\n",
    "    for index in range(len(list_of_tokens)):\n",
    "        masked_tokens=list(list_of_tokens[index])\n",
    "        masked_encodings=list(list_of_encoding[index])\n",
    "        random_indexes=set()\n",
    "\n",
    "        nb_tokens = len(masked_tokens)\n",
    "        nb_tokens_to_mask = int(nb_tokens*mask_rate)+1\n",
    "        while(nb_tokens_to_mask):\n",
    "            mask_sequence_length = random.randint(1,min(3,nb_tokens_to_mask))\n",
    "            start_pos = random.randint(0 ,nb_tokens-mask_sequence_length )\n",
    "            for i in range(start_pos,start_pos+mask_sequence_length):\n",
    "                random_indexes.add(i)\n",
    "                replace= random.random()\n",
    "                if replace >0.3 :\n",
    "                    masked_tokens[i]= \"[MASK]\"\n",
    "                    masked_encodings[i]= vocab_dict[\"[MASK]\"]\n",
    "                else :\n",
    "                    random_token = random.choice(candidate_tokens)\n",
    "                    masked_tokens[i]= random_token\n",
    "                    masked_encodings[i]= vocab_dict[random_token]\n",
    "            nb_tokens_to_mask-=mask_sequence_length\n",
    "            \n",
    "        result_tokens.append(masked_tokens)\n",
    "        result_encodings.append(masked_encodings)\n",
    "        result_indexes.append(list(random_indexes))\n",
    "    return result_tokens , result_encodings , result_indexes\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original           : he made my day\n",
      "original tokens    : ['he made ', 'my d', 'ay']\n",
      "masked tokens      : ['he made ', 'my d', '[MASK]']\n",
      "original encodings : [19716, 5347, 243]\n",
      "masked encodings   : [19716, 5347, 68]\n",
      "mask index         : [2]\n",
      "original           : احلامي بسيييييطة\n",
      "original tokens    : ['احل', 'امي ', 'بس', 'يييي', 'يط', 'ة']\n",
      "masked tokens      : ['احل', 'امي ', 'بس', 'يييي', '[MASK]', 'ة']\n",
      "original encodings : [13511, 10407, 2955, 15092, 6449, 20145]\n",
      "masked encodings   : [13511, 10407, 2955, 15092, 68, 20145]\n",
      "mask index         : [4]\n",
      "original           : عاودولي الصيف النفسية ماهياش جاهزة لتغليف الكتب و الpanier و ال autodicte\n",
      "original tokens    : ['عاود', 'ولي ', 'الصيف ', 'النفسي', 'ة', ' ما', 'هياش ', 'جاهز', 'ة', ' لت', 'غل', 'يف ', 'الك', 'تب', ' و ال', 'pa', 'ni', 'er', ' و ال', ' a', 'ut', 'od', 'ic', 'te']\n",
      "masked tokens      : ['عاود', 'ولي ', 'الصيف ', 'النفسي', 'ة', ' ما', 'هياش ', 'جاهز', '[MASK]', ' لت', 'غل', 'يف ', 'الك', 'تب', ' و ال', 'pa', 'ni', 'er', ' و ال', ' oc', 'ut', 'lanation', 'ic', '[MASK]']\n",
      "original encodings : [6165, 1970, 19497, 19860, 20145, 1845, 12589, 13995, 20145, 17953, 15261, 15342, 1918, 507, 8837, 369, 169, 104, 8837, 77, 287, 394, 122, 100]\n",
      "masked encodings   : [6165, 1970, 19497, 19860, 20145, 1845, 12589, 13995, 68, 17953, 15261, 15342, 1918, 507, 8837, 369, 169, 104, 8837, 7662, 287, 1095, 122, 68]\n",
      "mask index         : [19, 21, 23, 8]\n",
      "original           : عكس ما يقولوا فيروز في الليل خير\n",
      "original tokens    : ['عكس ', 'ما ي', 'قولوا ', 'في', 'رو', 'ز في ', 'الليل ', 'خير']\n",
      "masked tokens      : ['عكس ', '[MASK]', 'قولوا ', 'في', 'رو', ' نروح ', 'الليل ', 'خير']\n",
      "original encodings : [9068, 1646, 13262, 151, 355, 15640, 13437, 1871]\n",
      "masked encodings   : [9068, 68, 13262, 151, 355, 18400, 13437, 1871]\n",
      "mask index         : [1, 5]\n"
     ]
    }
   ],
   "source": [
    "#TEST\n",
    "batch = next(get_chunk(cleaned_data,4))\n",
    "tokens , encodings = tokenize_batch(batch ,new_vocab)\n",
    "masked_tokens ,masked_encodings ,  mask_indexes = mask_batch(tokens, encodings)\n",
    "for i in range(len(batch)):\n",
    "    print(f\"original           : {batch[i]}\")\n",
    "    print(f\"original tokens    : {tokens[i]}\")\n",
    "    print(f\"masked tokens      : {masked_tokens[i]}\")\n",
    "    print(f\"original encodings : {encodings[i]}\")\n",
    "    print(f\"masked encodings   : {masked_encodings[i]}\")\n",
    "    print(f\"mask index         : {mask_indexes[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# putting the model together\n",
    "\n",
    "DONE WITH MASKING HAMDELLAH\n",
    "\n",
    "NEXT STEP : build the model , to do this we are going to use the embadding model we made , add a big feed forword layer for now ,\n",
    "ofcourse its a bad approach cause theres no way other tokens will be able to know the context of the phrase cause there is no interection between tokens , but for the sake of testing lets see how it performs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building a simple model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DarjaBert(nn.Module):\n",
    "    def __init__(self , vocab_size , embad_dim , window_size):\n",
    "        super().__init__()\n",
    "        self.embadding_layer = DarjaEmbadding( vocab_size , embad_dim , window_size) # output B T D\n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=embad_dim , out_features=embad_dim*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=embad_dim*4 , out_features=embad_dim*8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=embad_dim*8 , out_features=embad_dim*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=embad_dim*4 , out_features=embad_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=embad_dim*2 , out_features=vocab_size),\n",
    "\n",
    "        )\n",
    "    def forward(self , batch):\n",
    "        embadded = self.embadding_layer(batch)\n",
    "        return self.linear_layer(embadded)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for faster training will move everything to gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize the model , loss function and optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(new_vocab)\n",
    "embad_dim=256\n",
    "window_size = 2048\n",
    "darja_modelv0 = DarjaBert(vocab_size,embad_dim,window_size).to(device)\n",
    "\n",
    "lossf = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(darja_modelv0.parameters() , lr=1e-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functionss\n",
    "\n",
    "#this function turn the target encodings into our target by masking everythin but our target encodings\n",
    "def build_target(encodings, mask_indexes_batch):\n",
    "    result = encodings.clone()  \n",
    "\n",
    "    for i, masked_indexes in enumerate(mask_indexes_batch):\n",
    "\n",
    "        for j in range(encodings.shape[1]):\n",
    "            if j not in masked_indexes:\n",
    "                result[i][j] = -100  # to get ignored by cross entropy loss function\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and finally , the training and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 16\n",
    "\n",
    "train_history = []\n",
    "test_history = []\n",
    "splits = cleaned_data.train_test_split(test_size=0.2)\n",
    "train_data = splits['train']\n",
    "test_data = splits['test']\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"training...\")\n",
    "    loss_table=[]\n",
    "    for iter ,batch in tqdm.tqdm(enumerate(get_chunk(train_data,chunk_lenght=batch_size))):  \n",
    "        \n",
    "        #n7adhro el batch\n",
    "        tokens , encodings = tokenize_batch(batch ,new_vocab ,trunc = window_size)\n",
    "\n",
    "        masked_tokens ,masked_encodings ,  mask_indexes = mask_batch(tokens, encodings)\n",
    "        train_batch = to_tensor(masked_encodings , new_vocab ,window_size).to(device)\n",
    "        target_batch = to_tensor(encodings ,  new_vocab ,window_size).to(device)\n",
    "\n",
    "        target_batch = build_target(target_batch , mask_indexes).to(device)\n",
    "\n",
    "        # nebdaw el train\n",
    "\n",
    "        darja_modelv0.train()\n",
    "\n",
    "        y_pred = darja_modelv0(train_batch)\n",
    "\n",
    "        loss = lossf(y_pred.view(-1, vocab_size), target_batch.view(-1))\n",
    "        loss_table.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        del y_pred, loss, target_batch ,train_batch\n",
    "        torch.cuda.empty_cache()\n",
    "        if iter % 50 ==0:\n",
    "            loss_avrg = sum(loss_table) / len(loss_table)\n",
    "            train_history.append(loss_avrg)\n",
    "            print(f\"train loss : {loss_avrg} \")\n",
    "            loss_table.clear()\n",
    "    if loss_table:\n",
    "        loss_avrg = sum(loss_table) / len(loss_table)\n",
    "        train_history.append(loss_avrg)\n",
    "        print(f\"train loss : {loss_avrg} \")\n",
    "        loss_table.clear()\n",
    "\n",
    "\n",
    "    print(\"testing...\")\n",
    "    test_loss_table = []\n",
    "\n",
    "    darja_modelv0.eval()\n",
    "    with torch.inference_mode() :\n",
    "        for iter ,batch in tqdm.tqdm(enumerate(get_chunk(test_data,chunk_lenght=batch_size))):  \n",
    "            \n",
    "            #n7adhro el batch\n",
    "            tokens , encodings = tokenize_batch(batch ,new_vocab ,trunc = window_size)\n",
    "\n",
    "            masked_tokens ,masked_encodings ,  mask_indexes = mask_batch(tokens, encodings)\n",
    "            test_batch = to_tensor(masked_encodings , new_vocab ,window_size).to(device)\n",
    "            target_batch = to_tensor(encodings ,  new_vocab ,window_size).to(device)\n",
    "\n",
    "            target_batch = build_target(target_batch , mask_indexes)\n",
    "\n",
    "            # nebdaw el test\n",
    "\n",
    "            y_pred = darja_modelv0(test_batch)\n",
    "\n",
    "            loss = lossf(y_pred.view(-1, vocab_size), target_batch.view(-1))\n",
    "\n",
    "\n",
    "            test_loss_table.append(loss.item())\n",
    "            #for lack of enaugh ressources , i had to add these line ( helps with ram exaughtion by deleting unused tensors , but causese slow issues \n",
    "            del y_pred, loss, target_batch ,test_batch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if iter % 50 == 0:\n",
    "                test_avg = sum(test_loss_table) / len(test_loss_table)\n",
    "                test_history.append(test_avg)\n",
    "                print(f\"test loss : {test_avg}\")\n",
    "                test_loss_table.clear()\n",
    "        if test_loss_table:\n",
    "            test_avg = sum(test_loss_table) / len(test_loss_table)\n",
    "            test_history.append(test_avg)\n",
    "            print(f\"test loss : {test_avg}\")\n",
    "            test_loss_table.clear()\n",
    "\n",
    "\n",
    "        print(f\"test loss : {loss.item()} \")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i had to stop it since the loss is not going down anymore , which makes total sense, whith this architecture there is no connection between different tokens , each token passes through the model on its own (due to the feedforward achitecture) with no context of the total phrase , there is no why it could predict the right token\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there for ,we need an architecture that could give ATTENTION to other tokens at the same time , and here we meet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE TRANSFORMER ARCHITECTURE RAAAAAAAAAAAAAAA\n",
    "\n",
    "## -specificaly THE TRANSFORMER ENCODER ARCHITECTURE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the encoder is made up of different complex blocks (aka self-attention heads, normalization layers , feedforword networks ) its a good approach to create a class for each one finaly creating a main block AKA an EncoderBlock \n",
    "\n",
    "then we can stack how ever many blocks we want in the main DarjaModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im not gonna lie you need to watch some videos and read some papers to understand the next part , i recommend 3Blue1brown play list https://youtu.be/eMlx5fFNoYc?si=20CD4vJSiSN6l0MV\n",
    "and reading the original transformer paper \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first component is a basic but crucial mathmetical equation \n",
    "Attention(Q, K, V ) = softmax(Q*K.T/√dk)*V\n",
    "it will be applied to each head of the multihead attention layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):  \n",
    "    def __init__(self ,K_dim ):\n",
    "        super().__init__()\n",
    "        self.K_dim = K_dim\n",
    "        self.sm = nn.Softmax(dim=-1) # bech napplico 3al last dimention (softmax by default value is dim= 1)\n",
    "    def forward(self   ,K ,Q, V) :\n",
    "\n",
    "        return self.sm((Q@K.transpose(-1 ,-2)) / self.K_dim**0.5)@V \n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the heart of the encoder , the multi-head-attention layer in simpler terms finds the relationship between each token with each other token in the phrase,  to find more relationships we devide the embadding layer on the number of heads , then we concatinate them all together to then be passed by a linear layer\n",
    "to prevent overfitting we add a dropout layer at the end of it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiAttentionHead(nn.Module):\n",
    "    def __init__(self ,head_num , embad_dim):\n",
    "        super().__init__()\n",
    "        self.head_num = head_num\n",
    "        self.embad_dim = embad_dim\n",
    "        self.head_dim = embad_dim//head_num\n",
    "        self.scaled_dot_attention = ScaledDotProductAttention(self.head_dim)\n",
    "\n",
    "        self.K = nn.Linear(in_features=embad_dim , out_features=embad_dim)\n",
    "        self.Q = nn.Linear(in_features= embad_dim , out_features=embad_dim)\n",
    "        self.V = nn.Linear(in_features= embad_dim, out_features=embad_dim)\n",
    "        self.ll = nn.Sequential(\n",
    "            nn.Linear( in_features=embad_dim , out_features=embad_dim) ,\n",
    "            nn.Dropout(0.05)\n",
    "        )\n",
    "    def  forward(self ,batch ):\n",
    "\n",
    "        K_res = self.K(batch)\n",
    "        Q_res = self.Q(batch)\n",
    "        V_res = self.V(batch)\n",
    "        K_res = K_res.reshape((K_res.shape[0] , K_res.shape[1] , self.head_num,   self.head_dim)).transpose(1 , 2) \n",
    "        Q_res = Q_res.reshape((Q_res.shape[0] , Q_res.shape[1] , self.head_num ,  self.head_dim)).transpose(1 , 2)\n",
    "        V_res = V_res.reshape((V_res.shape[0] , V_res.shape[1] ,  self.head_num , self.head_dim)).transpose(1 , 2)\n",
    "\n",
    "\n",
    "\n",
    "        attention_res = self.scaled_dot_attention(K_res , Q_res ,V_res)\n",
    "\n",
    "        #concat\n",
    "        concated = attention_res.transpose(1 ,2)\n",
    "        concated = concated.reshape((concated.shape[0] , concated.shape[1] , self.embad_dim) )\n",
    "\n",
    "        return self.ll(concated)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the multiheadattention and the ffn are sublayers , after each sublayer we pass the output and the original input through an add and normalize layer , its works as the following  \n",
    "x= original input to the sublayer  \n",
    "y = output of the sublayer  \n",
    "normalized = normalization(x+y) = ((x+y) - mean) / std        # standered deviation  \n",
    "result=  G*(x+y)+ B  \n",
    "with G and B being a learnable vectors of size embad_dim  \n",
    "for better results its recommended to initiat G as vector of one and B as vector of 0 because starting at a random value could cause this layer to distored the signal , its better to start with it having no effect then making it learn the best values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self ,  embad_dim ):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(embad_dim,) ,  requires_grad=True)\n",
    "        self.beta = nn.Parameter(torch.zeros(embad_dim,) ,  requires_grad=True)\n",
    "    def forward(self , original : torch.Tensor , sublayer_output):\n",
    "        add = original + sublayer_output\n",
    "        mean = add.mean(dim=-1,keepdim=True)\n",
    "        stand_div = add.std(dim=-1 , keepdim=True) + 1e-4 # just in case el std ta7 lel 0\n",
    "        result = (add - mean)/stand_div\n",
    "        return self.gamma * result + self.beta\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the final sublayer of the encoder is the ffn (a basic feedforwardnetwork) we pass our batch of dim (B,T,D) , augment it , pass it by an activation function , then reduce the dimention back to its original shape , and as usual add a dropout layer to fight against overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self  , embad_dim):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=embad_dim , out_features=embad_dim*4),\n",
    "            nn.GELU(),            \n",
    "            nn.Linear(in_features=embad_dim*4 , out_features=embad_dim),\n",
    "            nn.Dropout(0.05)\n",
    "        )\n",
    "\n",
    "    def forward(self,batch):\n",
    "        return self.ffn(batch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE ENCODER BLOCK RAAAAAAA  \n",
    "we merge all those layer and pass the data through each one , one by one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self , head_num , embad_dim):\n",
    "        super().__init__()\n",
    "        self.head_num = head_num\n",
    "        self.embad_dim = embad_dim\n",
    "        self.multiHeadAttention = MultiAttentionHead(head_num , embad_dim)\n",
    "        self.addnorm1 = LayerNorm(embad_dim)\n",
    "        self.ffn = FeedForward(embad_dim)\n",
    "        self.addnorm2 = LayerNorm(embad_dim)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        sublayer1_res =self.addnorm1(batch ,self.multiHeadAttention(batch))\n",
    "        return self.addnorm2(sublayer1_res , self.ffn(sublayer1_res))\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now , to build our model , we first add how ever many encoderblocks we want , and at the end , add the classification layer we want , for our base case , masked token classification  , our output will be of size vocab_dict , FOR EACH TOKEN :\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finaaallyyy , the darjaa modelll\n",
    "\n",
    "class DarjaBERT(nn.Module):\n",
    "    def __init__(self , vocab_size , embad_dim , window_size):\n",
    "\n",
    "        super().__init__()\n",
    "        self.embadding_layer = DarjaEmbadding( vocab_size , embad_dim , window_size) # output B T D\n",
    "        self.encoder1 = EncoderBlock(8 ,embad_dim)\n",
    "        self.encoder2 = EncoderBlock(8 ,embad_dim)\n",
    "        self.encoder3 = EncoderBlock(8 ,embad_dim)\n",
    "        self.encoder4 = EncoderBlock(8 ,embad_dim)\n",
    "        self.encoder5 = EncoderBlock(8 ,embad_dim)\n",
    "        self.encoder6 = EncoderBlock(8 ,embad_dim)\n",
    "        self.classification_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=embad_dim , out_features=embad_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=embad_dim , out_features=vocab_size),\n",
    "\n",
    "        )\n",
    "    def forward(self , batch):\n",
    "        return self.classification_layer(self.encoder6(self.encoder6(self.encoder4(self.encoder3(self.encoder2(self.encoder1(self.embadding_layer(batch))))))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as usual , before training , we should setup our model , loss function , and optimizer   \n",
    "btw those hyperparameters were chosen after ALOT OF TRIAL (IM TIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(new_vocab)\n",
    "embad_dim=256\n",
    "window_size = 512\n",
    "darja_model = DarjaBERT(vocab_size,embad_dim,window_size).to(device)\n",
    "\n",
    "lossf = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(darja_model.parameters() , lr=5e-5 , weight_decay=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "due to training locally on my not so powerfull but good enaugh rtx 4060 8 gb vram , training wont be so fast , it takes days , so to not lose progress (or more like not lose any more progress , (i lost alot :( ))) i updated the training loop to save progress every 1000 steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading old data \n",
      "check point found\n",
      "4\n",
      "355200\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:07,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.7198907550018614 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [01:49,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.686530454158783 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [02:31,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.71625403881073 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [03:19,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.6324666237831114 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [04:10,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.670189802646637 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [05:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.6284261322021485 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "701it [05:48,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.591155562400818 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [06:32,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.666933798789978 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "901it [07:16,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.69001323223114 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [08:01,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.678540961742401 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1101it [08:52,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.565786955356598 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1201it [09:43,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.685308656692505 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1301it [10:27,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.5639486384391783 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1401it [11:13,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.6582046270370485 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [11:57,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.6132857739925384 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [12:46,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.7072993326187134 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1701it [13:40,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.7324403071403505 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1801it [14:25,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.6427250516414644 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1901it [15:09,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.6598622965812684 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [15:55,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.5978610730171203 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2101it [16:38,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.580254554748535 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2200it [17:27,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.586145122051239 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2301it [18:13,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.583846504688263 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2401it [19:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.6424536538124084 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2501it [19:41,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.648628420829773 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2601it [20:23,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.6062947416305544 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2701it [21:05,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.7902157354354857 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2800it [21:53,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.689290990829468 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2900it [22:46,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.6289433526992796 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [23:34,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.5736806178092957 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3101it [24:19,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.5838480651378632 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3201it [25:05,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.60456027507782 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3301it [25:52,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.696858285665512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3401it [26:38,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.6436022305488587 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3500it [27:29,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.5700430810451507 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3600it [28:35,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.6276571416854857 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3701it [29:25,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.7021967720985414 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3801it [30:12,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.6377049350738524 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3901it [30:58,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.673709828853607 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [31:44,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.7468317568302156 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4101it [32:28,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.7456189465522765 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4201it [33:18,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 3.6075255465507508 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4240it [33:29,  2.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m train_batch = to_tensor(masked_encodings , new_vocab ,window_size).to(device)\n\u001b[32m     37\u001b[39m target_batch = to_tensor(encodings ,  new_vocab ,window_size).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m target_batch = \u001b[43mbuild_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_indexes\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# nebdaw el train\u001b[39;00m\n\u001b[32m     43\u001b[39m darja_model.train()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mbuild_target\u001b[39m\u001b[34m(encodings, mask_indexes_batch)\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(encodings.shape[\u001b[32m1\u001b[39m]):\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m masked_indexes:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m             result[i][j] = -\u001b[32m100\u001b[39m  \u001b[38;5;66;03m# to get ignored by cross entropy loss function\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 8\n",
    "\n",
    "splits = cleaned_data.train_test_split(test_size=0.2)\n",
    "train_data = splits['train']\n",
    "test_data = splits['test']\n",
    "\n",
    "try:\n",
    "    print(\"loading old data \")\n",
    "    checkpoint = torch.load(\"training_state.pt\")\n",
    "    print(\"check point found\")\n",
    "    train_history = checkpoint['train_history']\n",
    "    test_history = checkpoint['test_history']\n",
    "    start = checkpoint['start']\n",
    "    resume_epoch = checkpoint['epoch']\n",
    "    darja_model.load_state_dict(torch.load(\"darja_model.pt\"))\n",
    "    print(epoch)\n",
    "    print(start)\n",
    "\n",
    "except:\n",
    "    print(\"starting new \")\n",
    "    train_history = []\n",
    "    test_history = []\n",
    "    start = 0\n",
    "    resume_epoch = 0\n",
    "\n",
    "for epoch in range(resume_epoch, epochs):\n",
    "    print(\"training...\")\n",
    "    loss_table=[]\n",
    "    for iter ,batch in tqdm.tqdm(enumerate(get_chunk(train_data,chunk_lenght=batch_size , start=start))):  \n",
    "        \n",
    "        #n7adhro el batch\n",
    "        tokens , encodings = tokenize_batch(batch ,new_vocab ,trunc = window_size)\n",
    "\n",
    "        masked_tokens ,masked_encodings ,  mask_indexes = mask_batch(tokens, encodings)\n",
    "        train_batch = to_tensor(masked_encodings , new_vocab ,window_size).to(device)\n",
    "        target_batch = to_tensor(encodings ,  new_vocab ,window_size).to(device)\n",
    "\n",
    "        target_batch = build_target(target_batch , mask_indexes).to(device)\n",
    "\n",
    "        # nebdaw el train\n",
    "\n",
    "        darja_model.train()\n",
    "\n",
    "        y_pred = darja_model(train_batch)\n",
    "\n",
    "        loss = lossf(y_pred.view(-1, vocab_size), target_batch.view(-1))\n",
    "        loss_table.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \"\"\"del y_pred, loss, target_batch ,train_batch\n",
    "        torch.cuda.empty_cache()\"\"\"\n",
    "        #for lack of enaugh ressources , i had to add these line ( helps with ram exaughtion by deleting unused tensors , but causese slow issues \n",
    "        if iter!=0 and iter % 100 ==0:\n",
    "            loss_avrg = sum(loss_table) / len(loss_table)\n",
    "            train_history.append(loss_avrg)\n",
    "            print(f\"train loss : {loss_avrg} \")\n",
    "            loss_table.clear()\n",
    "        #added this part cause it takes a looonnng long time and  i kept losing all my progress\n",
    "        if iter!=0 and iter% 100==0:\n",
    "            #nsavio el progress every 1000 steps\n",
    "            torch.save({\n",
    "                'train_history': train_history,\n",
    "                'test_history': test_history,\n",
    "                'start': start + iter * batch_size,\n",
    "                'epoch': epoch\n",
    "            }, \"training_state.pt\")\n",
    "            torch.save(darja_model.state_dict(), \"darja_model.pt\")\n",
    "    if loss_table:\n",
    "        loss_avrg = sum(loss_table) / len(loss_table)\n",
    "        train_history.append(loss_avrg)\n",
    "        print(f\"train loss : {loss_avrg} \")\n",
    "        loss_table.clear()\n",
    "    print(\"testing...\")\n",
    "    test_loss_table = []\n",
    "\n",
    "    darja_model.eval()\n",
    "    with torch.inference_mode() :\n",
    "        for test_iter ,batch in tqdm.tqdm(enumerate(get_chunk(test_data,chunk_lenght=batch_size))):  \n",
    "            \n",
    "            #n7adhro el batch\n",
    "            tokens , encodings = tokenize_batch(batch ,new_vocab ,trunc = window_size)\n",
    "\n",
    "            masked_tokens ,masked_encodings ,  mask_indexes = mask_batch(tokens, encodings)\n",
    "            test_batch = to_tensor(masked_encodings , new_vocab ,window_size).to(device)\n",
    "            target_batch = to_tensor(encodings ,  new_vocab ,window_size).to(device)\n",
    "\n",
    "            target_batch = build_target(target_batch , mask_indexes)\n",
    "\n",
    "            # nebdaw el test\n",
    "\n",
    "            y_pred = darja_model(test_batch)\n",
    "\n",
    "            loss = lossf(y_pred.view(-1, vocab_size), target_batch.view(-1))\n",
    "\n",
    "\n",
    "            test_loss_table.append(loss.item())\n",
    "            del y_pred, loss, target_batch ,test_batch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if test_iter % 100 == 0:\n",
    "                test_avg = sum(test_loss_table) / len(test_loss_table)\n",
    "                test_history.append(test_avg)\n",
    "                print(f\"test loss : {test_avg}\")\n",
    "                test_loss_table.clear()\n",
    "        if test_loss_table:\n",
    "            test_avg = sum(test_loss_table) / len(test_loss_table)\n",
    "            test_history.append(test_avg)\n",
    "            print(f\"test loss : {test_avg}\")\n",
    "            test_loss_table.clear()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        print(f\"test loss : {loss.item()} \")\n",
    "    start = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7126"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6075255465507508"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_history[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5034353804588316"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_history[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot our training history to see the loss curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_history[10:])\n",
    "plt.title(\"Train Loss Over Time\")\n",
    "plt.xlabel(\"Checkpoint\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks yesser da5el ba3dho sooo lets smooth it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def smooth(data, window_size=15):\n",
    "    return [sum(data[i:i+window_size])/window_size for i in range(len(data)-window_size)]\n",
    "smoothed = smooth(train_history[10:], window_size=20)\n",
    "plt.plot(smoothed)\n",
    "plt.title(\"Smoothed Train Loss \")\n",
    "plt.xlabel(\"100 batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks great !\n",
    "lets test it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masked_tokens(model,masked_encodings ,batchof_mask_indexes ):\n",
    "    predictions = model(masked_encodings).argmax(dim=-1)\n",
    "   \n",
    "    result = []\n",
    "\n",
    "    for original_encodings ,mask_indexes , pred in zip(masked_encodings,batchof_mask_indexes , predictions):\n",
    "        encodings = pred.tolist()\n",
    "        original_encodings = original_encodings.tolist()\n",
    "        for j in mask_indexes:\n",
    "            original_encodings[j] = encodings[j]\n",
    "        result.append(decode(original_encodings))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original phrase : waaa bro wenek nestennew fyk men bekry \n",
      "masked phrase   : wacbro wenek nestennew fy[MASK]men bekryص\n",
      "prediction      : waa bro wenek nestennew fys men bekry[UNK]\n"
     ]
    }
   ],
   "source": [
    "phrase = \"waaa bro wenek nestennew fyk men bekry \"\n",
    "tokens , encodings = tokenize_batch([phrase] , new_vocab)\n",
    "masked_tokens , masked_encodings,mask_indexes = mask_batch(tokens , encodings, vocab_dict)\n",
    "input = to_tensor(masked_encodings ,new_vocab ,window_size)\n",
    "prediction = predict_masked_tokens(darja_model ,input , mask_indexes)\n",
    "print(f\"original phrase : {phrase}\")\n",
    "print(f\"masked phrase   : {\"\".join(masked_tokens[0])}\")\n",
    "print(f\"prediction      : {prediction[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original phrase : من احلى ما سمعت و ما بش نسمع في حياتي\n",
      "masked phrase   : [MASK]حلى[MASK]سمعت و ما بش نسمع في حياتي\n",
      "prediction      : والله حلى شنو سمعت و ما بش نسمع في حياتي\n"
     ]
    }
   ],
   "source": [
    "phrase = \"من احلى ما سمعت و ما بش نسمع في حياتي\"\n",
    "tokens , encodings = tokenize_batch([phrase] , new_vocab)\n",
    "masked_tokens , masked_encodings,mask_indexes = mask_batch(tokens , encodings, vocab_dict)\n",
    "input = to_tensor(masked_encodings ,new_vocab ,window_size)\n",
    "prediction = predict_masked_tokens(darja_model ,input , mask_indexes)\n",
    "print(f\"original phrase : {phrase}\")\n",
    "print(f\"masked phrase   : {\"\".join(masked_tokens[0])}\")\n",
    "print(f\"prediction      : {prediction[0]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion  \n",
    "the model has learned alot , it can understand the sementics behind our darija  and can understand relationships between tokens in our phrases , still not so good enaugh at predicting the missing tokens but  considering its **limited size (10 times smaller than bert base)** its expected , i believe i could have reached a better loss if i spent a bit more time collecting a higher quality data , but either way , our darja is unpredictable and unstable , each word could be written in multiple different ways , it would take a huggg amount of data + a bigger model to work better on this .\n",
    "either way its now ready for finetuning onto other task\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetuning\n",
    "in this section we will take the pretrained model , finetune it for other task , for exemple\n",
    "\n",
    "## darja sentiment analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first lets get the data , to do so we will create a custom dataset class (little overboard but seemed fun to try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset , DataLoader\n",
    "import pandas as pd\n",
    "class sentiment_dataset(Dataset):\n",
    "    def __init__(self ,  csv_path, transform =None):\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(csv_path ,usecols=[\"text\" ,\"label\"])\n",
    "\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        text = self.data.iloc[idx][\"text\"]\n",
    "        label =  self.data.iloc[idx][\"label\"] if self.data.iloc[idx][\"label\"]!=-1 else 2\n",
    "        return self.transform(text) , label\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we could apply a transformation function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sa_text(text):\n",
    "    text = re.sub(r'[^\\u0600-\\u06FFa-zA-Z0-9\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "sa_data = sentiment_dataset(\"SA_data.csv\",clean_sa_text)\n",
    "sa_train_data , sa_test_data  = train_test_split(sa_data , test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63000\n",
      "7000\n"
     ]
    }
   ],
   "source": [
    "print(len(sa_train_data))\n",
    "print(len(sa_test_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "greatt , lets create the model now , it will work as the following , we will add a CLS token at the begining of each sentence , and we will train the model to encode the class of the sentence in that token , so we can use it to classify the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darja_sa(nn.Module):\n",
    "    def __init__(self , base_model : nn.Module , embad_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(*list(base_model.children())[:-1]) #pretrained model outputs b t d\n",
    "\n",
    "        #usually we use this line to freeze the pretrained model , but since we didnt use cls in our original model we need to finetune it all to adjust the embading of cls token (very rookie mistake)\n",
    "        \"\"\"for param in self.model.parameters():\n",
    "            param.requires_grad = False\"\"\"\n",
    "        self.classifier = nn.Sequential(nn.Linear(embad_dim , embad_dim//2),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(embad_dim//2 ,3)\n",
    "                                        )\n",
    "    def forward(self, x):\n",
    "        res = self.model(x)\n",
    "        return self.classifier(res[:,0,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(new_vocab)\n",
    "embad_dim=256\n",
    "window_size = 512\n",
    "darja_model = DarjaBERT(vocab_size,embad_dim,window_size).to(device)\n",
    "darja_model.load_state_dict(torch.load(\"darja_model.pt\"))\n",
    "sa_model = Darja_sa(darja_model,embad_dim).to(device)\n",
    "lossf = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(sa_model.parameters() , lr=1e-5 , weight_decay=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now onto training , the principle is the following , we are going to add the cls token at the begining of each phrase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(chunk , vocab_dict ,trunc = float(\"inf\"), ignore_unk = False ):\n",
    "    tokens = []\n",
    "    encodings = []\n",
    "    for text in chunk:\n",
    "        token , encoding = tokenize(text , vocab_dict=vocab_dict, ignore_unknown = ignore_unk)\n",
    "        token.insert(0,\"[CLS]\")\n",
    "        encoding.insert(0,vocab_dict[\"[CLS]\"])\n",
    "        if len(encoding)> trunc:\n",
    "            encoding = encoding[:trunc]\n",
    "            token = token[:trunc]\n",
    "        tokens.append(token)\n",
    "        encodings.append(encoding)\n",
    "    return tokens, encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading old data \n",
      "check point found\n",
      "6\n",
      "1900\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1661it [00:00, 16455.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.243794783949852 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1969it [00:14, 137.28it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.30402449978625073 \n",
      "testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:11, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss : 0.5868487616281698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "203it [00:21, 11.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss : 0.5875232611596585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "219it [00:23,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss : 0.5779734767145581\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:22,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.3153160941777843 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:44,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.33202811107039454 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [01:06,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.34136503145098684 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:29,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.3161483037471771 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [01:51,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.3310712774097919 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [02:12,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.3399214096367359 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "701it [02:32,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.3408610686659813 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:54,  4.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.34104094743728636 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "901it [03:16,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.34872049853205683 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [03:37,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.32087556131184103 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1101it [03:57,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.331924661397934 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1201it [04:20,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.34544093802571296 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1301it [04:42,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.32330946177244185 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1401it [05:05,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.34064950242638586 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1500it [05:28,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.33980477079749105 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [05:50,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.34503229409456254 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1700it [06:11,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.33337887227535246 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1801it [06:32,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.30598163202404977 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1901it [06:55,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.33860808953642846 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1969it [07:08,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.3162865338518339 \n",
      "testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:12, 10.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss : 0.616584067297454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "202it [00:22, 12.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss : 0.6128488346934319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "219it [00:24,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss : 0.5917740927802192\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:22,  3.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[241]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m#n7adhro el batch\u001b[39;00m\n\u001b[32m     34\u001b[39m tokens , encodings = tokenize_batch(texts ,new_vocab ,trunc = window_size)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m labels=\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m train_batch = to_tensor(encodings ,  new_vocab ,window_size).to(device)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# nebdaw el train\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "sa_train_dataloader = DataLoader(sa_train_data,batch_size=batch_size,shuffle = True)\n",
    "sa_test_dataloader = DataLoader(sa_test_data,batch_size=batch_size,shuffle = True)\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"loading old data \")\n",
    "    checkpoint = torch.load(\"training_state_sa.pt\")\n",
    "    print(\"check point found\")\n",
    "    train_history = checkpoint['train_history']\n",
    "    test_history = checkpoint['test_history']\n",
    "    start = checkpoint['start']\n",
    "    resume_epoch = checkpoint['epoch']\n",
    "    sa_model.load_state_dict(torch.load(\"darja_model_sa.pt\"))\n",
    "    print(resume_epoch)\n",
    "    print(start)\n",
    "\n",
    "except:\n",
    "    print(\"starting new \")\n",
    "    train_history = []\n",
    "    test_history = []\n",
    "    start = 0\n",
    "    resume_epoch = 0\n",
    "\n",
    "for epoch in range(resume_epoch, epochs):\n",
    "    print(\"training...\")\n",
    "    loss_table=[]\n",
    "    for iter ,batch in tqdm.tqdm(enumerate(sa_train_dataloader)):  \n",
    "        if iter<start:\n",
    "            continue\n",
    "        texts , labels = batch\n",
    "        #n7adhro el batch\n",
    "        tokens , encodings = tokenize_batch(texts ,new_vocab ,trunc = window_size)\n",
    "        labels=labels.to(device)\n",
    "        train_batch = to_tensor(encodings ,  new_vocab ,window_size).to(device)\n",
    "\n",
    "        # nebdaw el train\n",
    "\n",
    "        sa_model.train()\n",
    "\n",
    "        y_pred = sa_model(train_batch)\n",
    "        loss = lossf(y_pred, labels)\n",
    "        loss_table.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter!=0 and iter % 100 ==0:\n",
    "            loss_avrg = sum(loss_table) / len(loss_table)\n",
    "            train_history.append(loss_avrg)\n",
    "            print(f\"train loss : {loss_avrg} \")\n",
    "            loss_table.clear()\n",
    "        #added this part cause it takes a looonnng long time and  i kept losing all my progress\n",
    "        if iter!=0 and iter% 100==0:\n",
    "            #nsavio el progress every 1000 steps\n",
    "            torch.save({\n",
    "                'train_history': train_history,\n",
    "                'test_history': test_history,\n",
    "                'start':  iter ,\n",
    "                'epoch': epoch\n",
    "            }, \"training_state_sa.pt\")\n",
    "            torch.save(sa_model.state_dict(), \"darja_model_sa.pt\")\n",
    "    if loss_table:\n",
    "        loss_avrg = sum(loss_table) / len(loss_table)\n",
    "        train_history.append(loss_avrg)\n",
    "        print(f\"train loss : {loss_avrg} \")\n",
    "        loss_table.clear()\n",
    "    print(\"testing...\")\n",
    "    test_loss_table = []\n",
    "\n",
    "    sa_model.eval()\n",
    "    with torch.inference_mode() :\n",
    "        for test_iter ,batch in tqdm.tqdm(enumerate(sa_test_dataloader)):  \n",
    "            \n",
    "            texts , labels = batch\n",
    "            #n7adhro el batch\n",
    "            tokens , encodings = tokenize_batch(texts ,new_vocab ,trunc = window_size)\n",
    "            labels=labels.to(device)\n",
    "            test_batch = to_tensor(encodings ,  new_vocab ,window_size).to(device)\n",
    "\n",
    "\n",
    "            y_pred = sa_model(test_batch)\n",
    "            loss = lossf(y_pred, labels)\n",
    "\n",
    "\n",
    "            test_loss_table.append(loss.item())\n",
    "\n",
    "\n",
    "            if  test_iter!=0 and test_iter % 100 == 0:\n",
    "                test_avg = sum(test_loss_table) / len(test_loss_table)\n",
    "                test_history.append(test_avg)\n",
    "                print(f\"test loss : {test_avg}\")\n",
    "                test_loss_table.clear()\n",
    "        if test_loss_table:\n",
    "            test_avg = sum(test_loss_table) / len(test_loss_table)\n",
    "            test_history.append(test_avg)\n",
    "            print(f\"test loss : {test_avg}\")\n",
    "            test_loss_table.clear()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    start = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the test accuracy is not improving anymore, so lets stop it there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, dataloader, vocab, window_size):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm.tqdm(dataloader):\n",
    "            texts, labels = batch\n",
    "            tokens, encodings = tokenize_batch(texts, vocab, trunc=window_size)\n",
    "            inputs = to_tensor(encodings, vocab, window_size).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Evaluating final accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1969/1969 [03:45<00:00,  8.72it/s]\n",
      "100%|██████████| 219/219 [00:23<00:00,  9.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final Train Accuracy: 89.62%\n",
      "✅ Final Test Accuracy: 75.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"🔍 Evaluating final accuracy...\")\n",
    "\n",
    "train_acc = evaluate_accuracy(sa_model, sa_train_dataloader, new_vocab, window_size)\n",
    "test_acc = evaluate_accuracy(sa_model, sa_test_dataloader, new_vocab, window_size)\n",
    "\n",
    "print(f\"✅ Final Train Accuracy: {train_acc * 100:.2f}%\")\n",
    "print(f\"✅ Final Test Accuracy: {test_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yeah not so bad considering my setup , hopefully on my next tries , with a bit of planning , better data and better hardware , this could be a better model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thank you for following along , if you didnt understand anything , feel free to contact me : https://www.linkedin.com/in/mahdi-ben-ameur-5089ba240/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
